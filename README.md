# Let's translate a text

Test the power of cascading machine learning models

In early 90s, statistical machine translation (SMT) became popular. SMT systems used statistical models to learn how words and phrases in one language correspond to words and phrases in another language. These systems required large amounts of bilingual data and were limited by their inability to capture the meaning and context of a sentence.

In the early 2010s, neural machine translation (NMT) emerged as a breakthrough in machine translation. NMT systems use artificial neural networks to model the relationships between words and phrases in different languages. NMT models can capture complex language structures and are able to produce more fluent and accurate translations than SMT systems.

In recent years, transformer-based architectures like the GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) have shown great success in natural language processing (NLP), including language translation. These models are trained on massive amounts of data and can capture complex patterns in language, leading to highly accurate translations.

Overall, machine translation has made significant progress over the past few decades, with NMT and transformer-based models showing great promise for accurate and fluent translations.

In this project we will run trough a neural network architecture to translate from French to English
